---
title: "Notes on Zipf's Law"
author: "Gennady Khvorykh, [followbigdata.com](http://followbigdata.com)"
date: "Saturday, April 09, 2016"
output: 
  html_document:
    toc: true
---

```{r}
require(zipfR)
```

### Terms
* `V` is vocabulary size (number of types)
* `N` is sample size (number of tokens)
* `Vm` is number of types tha have frequency m
* `hapax legomena` is the number of types that occur only once in the corpus ($V_1$)

`Frequency spectrum` summerizes how many times (m) the number of distinc types (Vm) occurs.

```{r}
data(ItaRi.spc)
spec <- ItaRi.spc
rm(ItaRi.spc)
spec
```

### Vizualization
```{r}
plot(spec)

# Vizualization on ligarithmic base
plot(spec, log = "x", xlab = "log10(m)") # only x is on ligarithmic base

# Subset the first 50 frequency classes
spec.50 <- spec[spec$m < 50, ]
spec.50[c("m", "Vm")] <-  log10(spec.50[c("m", "Vm")])

fit <- lm(Vm ~ m, spec.50)
with(spec.50, {
  plot(m, Vm, main = "Frequency Spectrum", xlab = "log10(m)", ylab = "log10(Vm)")
  abline(fit, col = "blue")
  })
```

### Measure of Productivity
Frequency spetrum plots tell about the nature of a process. 

High proportion of `hapax legomena` and other low frequency classes indicates the process being productive.  

The higher the productivity of the process, the higher the  chances to encounter new types of tokens, if  we  were  to  sample  more  tokens of the same category.

```{r}
Vm(spec, 1)/N(spec)
```

The vocabulary size increases as sample size increases. How rapid does vocabulary size grow?

### Vocabulary growth curves (VGC)

```{r}
data(ItaRi.emp.vgc)
head(ItaRi.emp.vgc)
```

VGC reports vocabulary size (number of types, V) as a function of sample size (number of tokens, N). For example, after N = 500000, V = `r V(ItaRi.emp.vgc)[N(ItaRi.emp.vgc) == 500000]`, while for the whole dataset V = `r V(spec)`. The number of samples included in VGC can be seen from summary()

```{r}
summary(ItaRi.emp.vgc)
```

The most interesting frequency class is *hapax legomena*. It is often included in VGC data. To add $V_1$ curve to VGC plot, specify `add.m=1` 

```{r}
plot(ItaRi.emp.vgc, add.m = 1)
```

### How to check the assumtion of randomness?

To invalidate the statistical model, compare the empirical VGC with interpolated one. The different shapes of the empirical and expected curves is the source of concern about the validity of randomness assumptions.  

Smothed curve can be obtained with `binomial interpolation`. It can be done directly from frequency spectrum with `vgc.interp()`.

```{r}
inter.VGC <- vgc.interp(spec, N(ItaRi.emp.vgc))
plot(ItaRi.emp.vgc, inter.VGC, legend = c("observed", "interpolated"))

```

### How to extrapolate V?

Toolkit of `zipfR` package supports **3 classes of Large-Number-of-Rare-Events (LNRE) models**: Generalized Inverse Gauss Poisson (lnre.gigp), Zipf-Mandelbrot (
lnre.zm) and fnite Zipf-Mandelbrot (lnre.fzm).

```{r}
fit <- lnre("fzm", spec, exact = F)
expected <- lnre.spc(fit, N(fit))
plot(spec, expected, legend = c("observed", "fZM"))
```

The fZM model can be used to estimate $V$ and $V_m$ at arbitrary sample size. 

```{r}
expected <- lnre.vgc(fit, (1:100)*28e3)
plot(ItaRi.emp.vgc, expected, N0 = N(fit), legend = c("observed", "fZM"))
```


### Comparing VGC of different categories

It is interesting to compare two word formation process. Which one is more productive? For example, adjectives or nouns in Brown corpus? Since adjective-sample is smaller, we extrapolate it to the size of noun-sample.   

```{r}
data(BrownAdj.emp.vgc)
data(BrownAdj.spc)
data(BrownNoun.emp.vgc)

fit <- lnre("fzm", BrownAdj.spc, exact = F) # fit the model
BrownAdj.exp.vgc <- lnre.vgc(fit, N(BrownNoun.emp.vgc)) # predict the values
plot(BrownAdj.exp.vgc, BrownNoun.emp.vgc, legend = c("adj", "noun"))
```

### Estimating the proportion of OOV types and tokens given a fixed size lexicon

Consider the Brown corpus. 

```{r}
data(Brown100k.spc)

V(Brown100k.spc)

plot(Brown100k.spc)

```

Hapax legomena consists of `r Vm(Brown100k.spc, 1)/N(Brown100k.spc)*100` percent of the sample size. We can afford to considere $V_1$ as OOV. But what will happen when on a larger input? 

### References
* Marco Baroni, Stefan Evert, [The zipfR package for lexical statistics: A tutorial introduction](http://zipfr.r-forge.r-project.org/materials/zipfr-tutorial.pdf), 2014.

* [zipfR  website](http:/zipfR.r-forge.r-project.org/)